%\documentclass[a4paper, 12pt, twoside, openright, titlepage, enabledeprecatedfontcommands]{scrbook}
\documentclass[a4paper, 12pt, twoside, openright, titlepage]{book}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage{scrlayer-scrpage} % stili pagina per il frontespizio

\usepackage{booktabs}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{subfig}

\usepackage[beramono, pdfspacing, dottedtoc,linedheaders]{classicthesis}
%\usepackage{arsclassica}
\usepackage{amsmath, amsfonts} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting

\usepackage{amssymb, verbatim,mathtools,needspace,enumitem,etoolbox,graphicx, physics,microtype,afterpage,bigints,gensymb,tabularx, comment}

\usepackage[top=2.5cm, bottom=2.5cm, inner=4cm, outer=4cm, right=2.5cm, centering]{geometry}
\usepackage{emptypage}
%\usepackage{fancyhdr}



% Interlinea
\linespread{1.5}


\usepackage{csquotes} % per le citazioni "in blocco"
\usepackage[square]{natbib} % bibliografia con pacchetto biblatex (https://ctan.org/pkg/biblatex?lang=en)
\bibliographystyle{apsrev4-1}
%\bibliographystyle{apsrev4-2-author-truncate}
%\bibliographystyle{plain}
%\bibliographystyle{apacite}
\setcitestyle{numbers}
\appto{\bibsetup}{\raggedright}

\usepackage{titlesec} % per la formattazione dei titoli delle sezioni, capitoli etc.
\usepackage{float} % per il posizionamento delle immagini

\usepackage{listings} % per il codice di programmazione
% Fonte https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings. Per la lista di sintassi riconosciute.
\renewcommand{\lstlistingname}{Code}% Listing -> Codice
\usepackage{xcolor}  % stile del codice
\usepackage{hyperref}

\graphicspath{{../images}}


%%fancy stuff
%%\pagestyle{fancy}
%
%% Redefine \chaptermark and \sectionmark to remove prefixes
%\renewcommand{\chaptermark}[1]{\markboth{\ #1}{}}
%\renewcommand{\sectionmark}[1]{\markright{\thesection $\,\,$ \ #1}}
%
%\fancyhf{} % Clear all header and footer fields
%\fancyhead[LE,RO]{\small \thepage} % Page number on the outer corners
%\fancyhead[RE]{\small \spacedlowsmallcaps \leftmark} % Chapter title on the right of even pages (LE)
%\fancyhead[LO]{\small \spacedlowsmallcaps \rightmark} % Section title on the left of odd pages (RO)
%\setlength{\headsep}{11pt}
%

\title{Annual Progression Review}
\author{Cecilia Maria Fabbri}
\date{1 Steptember 2025}	

\begin{document}
\bibliographystyle{plain}
% Frontespizio

\maketitle


% Fine frontespizio
%\thispagestyle{empty}
%\clearpage

\frontmatter

\tableofcontents
%\thispagestyle{empty}

%\listoffigures
%\thispagestyle{empty}

%\listoftables

%\clearpage

\begin{chapter}{Literature Review}
\begin{comment}
- intro gw super quick -> we have a signal! (add if you have space: good candidates to detect are binary systems, detectors, current catalogue description)
- data analysis for single event
- data analysis for populations
- sbi basics, single event
- sbi population
- growing pains (accuracy requirements)
--------------------
\end{comment}

Gravitational waves (GWs) are perturbations of spacetime generated by mass distributions with non-null second derivative of the quadrupole mass-moment.
Binary systems of stellar objects are exemplary GW emitters
% change, too similar to thesis
and the more compact the object, the bigger
% in amplitude
the wave produced, making the system more suitable for detection. 

The LIGO-Virgo-KAGRA (LVK) collaboration uses a system of interferometers to detect gravitational waves.
%  and they build up catalogues of gw events.
The latest complete catalogue published by the LVK collaboration is the third gravitational-wave transient catalogue (GWTC-3) and contains about a hundred signals generated by the coalescence of binaries of neutron stars (NSs) and stellar-mass black holes (BHs).

When LVK detect an astrophysical signal, they analyze the data to infer the properties of the event, such as the masses of the objects in the binary, their spins, the distance from the observer and its sky localization.
The statistical framework used for individual event analysis is Bayesian inference.
With the number of events growing, it becomes crucial to do population analyses, as they allow to set further constraints on the astrophysics underlying stellar objects.
To do so, one uses Hierarchical Bayesian statistics.

The models used have multi-dimensional parameters, resulting in probability distributions computationally prohebitive. 
% this sentence is strange, say the problem is the integral
Traditional methods rely on stochastic sampling methods, such as Markov Chain Monte Carlo (MCMC) or Nested sampling, to infer the probability distribution.
However powerful, these methods are expensive, as probability distributions have to be evaluated iteratively million of times for each individual event analysis. 
% look for citations
% do we have numbers for populations?
Each evaluation requires to generate a waveform on the fly, which heavily contributes to the overall costs.
% check if true
% si puo dire on the fly su un documento di questo tipo?
Machine learning approaches have the advantage of moving most of the costs 
%offline
in the training stage, dramatically speeding up inference time, which is crucial for multimessanger astronomy.
Several attempts have been made in this direction, from architectures that model the noise more realistically than traditional methods, to architectures that employ complex physical models without an analytical expression, which is unfeasible with stochastic samplers.
% sei sicura?

\begin{comment}
(nothing technical: that comes later)
- one uses bayesian stat to analyze individual events
- one uses hierarchical to analyze populations
- an alternative approach is simulation based inference
- advantages for individual events
- advantages for populations
\end{comment}

% add some data description? It's nice for the physics but not enough space
\begin{section}{Bayesian statistics}

Let us refer to the vector of binary parameters as $\theta$. It consists of intrinsic parameters, such as the masses and the spins, and extrinsic parameters, such as the redshift and the binary orientation.
The goal of the analysis is to infer the posterior distribution $p(\theta|d)$, which associates to each point in the binary-parameter space the probability of that binary to generate the observed data $d$. 
Observed data is the sum of the astrophysical signal, which is assumed to be deterministic and a function of binary-parameters, and a noise realization in the detector, which is assumed to be stochastic.
To infer the posterior one needs a prior $\pi(\theta)$ and a Likelihood $\mathcal{L}(d|\theta)$.
The prior is the probability distribution that an event with parameters $\theta$ takes place in the Universe. 
A possible approach to build is based on theory or previous experiments .
Nevertheless, the preferred approach is often conservative, and uninformative priors are used, such as priors with constant probability in the binary-parameters dominium and zero outside.
The Likelihood is the probability in the data space, given $\theta$ parameters.
To evaluate the Likelihood one needs to generate waveforms for $\theta$ and a noise model.
To build the noise model one often assumes that noise is stationary and Gaussian.
% non mi piace questa frase
Bayes theorem links the posterior to the prior and the Likelihood:
\begin{equation}
\label{eq: post}
p(\theta|d) = \frac{ \pi(\theta) \times \mathcal{L}(d|\theta)}{p(d)}.
\end{equation}
% non mi piace links in questo contesto
The term $p(d)$ is called evidence, and it is the probability of observing the data $d$ under the assumed model, which is often general relativity.
The evidence is defined as
\begin{equation}
p(d) = \int\textrm{d}\theta \pi(\theta) \times \mathcal{L}(d|\theta).
\end{equation}
The evidence is a normalization factor and does not influence the posterior shape.
For this reason one does not compute it during inference, except when doing model comparison.


\end{section}


\begin{section}{Hierarchical Bayesian statistics}
The goal of population analysis is to infer how binary parameters are distributed at the population level. 
To do so, one parametrizes the distribution of binary parameters with the population parameters $\lambda$, that will determine the shape of the distribution. 
Examples of hyperparameters are the slope of the binary black hole (BBH) mass distribution, the minimum and maximum mass of stellar-mass black holes.
The prior on binary parameters is conditioned on the population parameters, and we refer to it as the population model $p_{\rm pop}(\theta|\lambda)$.
For the analysis one needs the prior on population parameters, called hyperprior, $\pi(\lambda)$.

The choice of the population model is crucial, and several approaches have been proposed to construct it.
The state of the art are parametric models, which are based on easy to evaluate parametric forms built based on a qualitative astrophysical motivation.
Other approaches move towards highly flexible models, called semi-parametric or non-parametric models, where the hyperparameters are for instance spline perturbations. These models allow for data-driven fits but the results are difficult to interpret physically.
Moving towards astrophysical approaches, other attempts include comparing data with the output of simulations that synthesize binary populations.

In population analysis one needs to take into account selection effects.
Since the events have different probabilities to be detected, the observed population is biased.
An event is considered detectable if data exceeds some threshold.
The detection probability for an event is thus defined as
\begin{equation}
p_{\rm det}(\theta) = \int_{d>treshold} \textrm{d}d p(d|\theta).
\end{equation}

Let us refer to the dataset of observed events as $d$ and to individual events as $d_{i}$, with $i=1, ..., N_{\rm obs}$, where $N_{\rm obs}$ number of observed events.
The posterior for the astrophysical distribution, called hyperposterior, is:
\begin{equation}
\label{eq: hyperpost}
p(\lambda|d) = \frac{\pi(\lambda)}{p(d)} \prod_{i=1}^{N_{\rm obs}}\frac{\int \textrm{d}\theta \mathcal{L}(d_{i}|\theta)p_{\rm pop}(\theta|\lambda)}{\int \textrm{d}\theta p_{\rm pop}(\theta|\lambda)p_{\rm det}(\theta)},
\end{equation}
with $\mathcal{L}(d_{i}|\theta)$ the likelihood of individual events in Eq.~\ref{eq: post}.
The term $p(d)$ is the hyperevidence, and it is the probability of the observed dataset under the assumed population model, i.e.:
\begin{equation}
p(d)=\int \textrm{d}\lambda'  \pi(\lambda') \prod_{i=1}^{N_{\rm obs}} \mathcal{L}(d_{i}|\lambda'),
\end{equation}
with the likelihood respect to the hyperparameters $\mathcal{L}(d_{i}|\lambda')$ being linked to the individual-event likelihood through
\begin{equation}
\mathcal{L}(d_{i}|\lambda') = \int \textrm{d}\theta\mathcal{L}(d_{i}|\theta)p_{\rm pop}(\theta|\lambda).
\end{equation}
The hyperevidence is often computed in population analysis to compare different population models and their goodness in describing the observed dataset. 

Population analysis is divided in two steps.
First, individual events are analyzed with Bayesian statistics. 
Then, one uses the individual-event posteriors and the assumed population model to infer the hyperposterior in Eq.~\ref{eq: hyperpost}.

\end{section}



\begin{section}{Simulation based inference}
%Traditional methods that use stochastic sampling infer the posterior by (i) drawing events randomly from the prior and (ii) evaluating the Likelihood.
%One can use deep learning to model complex probability distributions.
Simulation-based inference (SBI) is a deep learning technique to approximate a probability distribution using simulated data.
I am working with the DINGO code, which uses SBI to model a posterior.
To simulate data, we need to draw samples from the prior and the likelihood.
Contrarily to traditional methods, the likelihood is not evaluated and an analytical expression is not necessary. 
In principle, more complex models can be used, for instance dropping the stationary and gaussian noise assumption or using waveforms from simulations.


Normalizing flows are often used in SBI to approximate complex distributions.
A normalizing flow is a mapping from a simple $\pi(u)$ to an approximation of the posterior $q_{\phi}(\theta|d)$, with $\phi$ parameters of the mapping. 
The mapping $f_{\phi}(u)$ must be invertible and with a simple Jacobian determinant, so that:
\begin{equation}
\label{eq: nf}
q_{\phi}(\theta|d) = \pi(f_{\phi}^{-1}(\theta))|\textrm{det}J_{f_{\phi}}^{-1}|.
\end{equation}
% long |
% what is the dependence of the flow on data?
% add phi and explain
To evaluate $q_{\phi}(\theta|d)$ one uses Eq.~\ref{eq: nf}. Drawing samples from $q_{\phi}(\theta|d)$ is equivalent to drawing $u\sim\pi(u)$ and transforming $\theta=f_{\phi}(u)$.
Often one chooses the base distribution $\pi(u)$ is a multivariate standard normal.
In the case of DINGO code, the flow $f_{\phi}$ is a sequence of transforms $f_{\phi}^{(i)}$, so that
\begin{equation}
f_{\phi}^{i}(u) = u_{i} if i\leq d/2, c_{i}(u_{i}; u_{1:d, d}) if i> d/2,
\end{equation}
so that half of the latent variables $u$ are unchanged while the other half is transformed.
At each $f_{\phi}^{(i)}$ the indices of the $u$ that transform and that do not are permuted.
The transformation is the neural spline coupling transform $c_{i}$, whose parameters depend on the unchanged $u_{i}$ and on data strain $d$.

During training the neural network learns the mapping by learning the $\phi$ parameters that minimize a chosen loss function.

% loss functions
The loss function used in DINGO is:
\begin{equation}
L = \frac{-1}{N}
\end{equation}

% mettici una graffona
% say explicetly that the flow is the network
CAPIRE MEGLIO E RISPIEGARE.








- we need to draw from the likelihood -> can use non-analytical models
- faster because we do all at the beginning and then tada (gia' detto nell'intro)



\end{section}















\end{chapter}

\begin{chapter}{Completed Work}
a descripiton of work completed to date
- small project description

Transfer learning is a series of techniques used in machine learning to transfer knowledge from a neural network to another.
Each time one modifies a neural network, for instance changing the model assumed in the analysis, the neural network needs to be trained again. 
The goal of transfer learning is to fine tune a new training run using some knowledge acquired during a previous training.
These strategies allow to reduce training time, and, more importantly, could even improve the results respect to training the new run without any information from previous trainings. 




\end{chapter}

\begin{chapter}{Work Plan}
a plan of work for the next 12 months
- pop growing pains for sure
- we talked about using astrophysical models as pop models. We need to understand better. Surely something more astro
- another idea is to keep developing the current project (NR simulations, beyond GR models)
\end{chapter}

\begin{chapter}{Personal Development Plan}
Copy of personal development plan summary and a statement of progress made towards those training goals
\end{chapter}

\appendix
%
%\pagestyle{fancy}
%
%\fancyhf{} % Clear all header and footer fields
%\fancyhead[LE,RO]{\small \thepage} % Page number on the outer corners
%\fancyhead[RE]{\small \spacedlowsmallcaps \leftmark} % Chapter title on the right of even pages (LE)
%\fancyhead[LO]{\small \spacedlowsmallcaps \leftmark}
%

\backmatter
%%BIBLIOGRAPHY
\bibliography{thesis}

\end{document}
