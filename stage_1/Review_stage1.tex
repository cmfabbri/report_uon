%\documentclass[a4paper, 12pt, twoside, openright, titlepage, enabledeprecatedfontcommands]{scrbook}
\documentclass[a4paper, 12pt, twoside, openright, titlepage]{book}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage{scrlayer-scrpage} % stili pagina per il frontespizio

\usepackage{booktabs}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{subfig}

\usepackage[beramono, pdfspacing, dottedtoc,linedheaders]{classicthesis}
%\usepackage{arsclassica}
\usepackage{amsmath, amsfonts} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting

\usepackage{amssymb, verbatim,mathtools,needspace,enumitem,etoolbox,graphicx, physics,microtype,afterpage,bigints,gensymb,tabularx, comment}

\usepackage[top=2.5cm, bottom=2.5cm, inner=4cm, outer=4cm, right=2.5cm, centering]{geometry}
\usepackage{emptypage}
%\usepackage{fancyhdr}



% Interlinea
\linespread{1.5}


\usepackage{csquotes} % per le citazioni "in blocco"
\usepackage[square]{natbib} % bibliografia con pacchetto biblatex (https://ctan.org/pkg/biblatex?lang=en)
\bibliographystyle{apsrev4-1}
%\bibliographystyle{apsrev4-2-author-truncate}
%\bibliographystyle{plain}
%\bibliographystyle{apacite}
\setcitestyle{numbers}
\appto{\bibsetup}{\raggedright}

\usepackage{titlesec} % per la formattazione dei titoli delle sezioni, capitoli etc.
\usepackage{float} % per il posizionamento delle immagini

\usepackage{listings} % per il codice di programmazione
% Fonte https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings. Per la lista di sintassi riconosciute.
\renewcommand{\lstlistingname}{Code}% Listing -> Codice
\usepackage{xcolor}  % stile del codice
\usepackage{hyperref}

\graphicspath{{../images}}


%%fancy stuff
%%\pagestyle{fancy}
%
%% Redefine \chaptermark and \sectionmark to remove prefixes
%\renewcommand{\chaptermark}[1]{\markboth{\ #1}{}}
%\renewcommand{\sectionmark}[1]{\markright{\thesection $\,\,$ \ #1}}
%
%\fancyhf{} % Clear all header and footer fields
%\fancyhead[LE,RO]{\small \thepage} % Page number on the outer corners
%\fancyhead[RE]{\small \spacedlowsmallcaps \leftmark} % Chapter title on the right of even pages (LE)
%\fancyhead[LO]{\small \spacedlowsmallcaps \rightmark} % Section title on the left of odd pages (RO)
%\setlength{\headsep}{11pt}
%

\title{Annual Progression Review}
\author{Cecilia Maria Fabbri}
\date{1 Steptember 2025}	

\begin{document}
\bibliographystyle{plain}
% Frontespizio

\maketitle


% Fine frontespizio
%\thispagestyle{empty}
%\clearpage

\frontmatter

\tableofcontents
%\thispagestyle{empty}

%\listoffigures
%\thispagestyle{empty}

%\listoftables

%\clearpage

\begin{chapter}{Literature Review}
\begin{comment}
- intro gw super quick -> we have a signal! (add if you have space: good candidates to detect are binary systems, detectors, current catalogue description)
- data analysis for single event
- data analysis for populations
- sbi basics, single event
- sbi population
- growing pains (accuracy requirements)
--------------------
\end{comment}

Gravitational waves (GWs) are perturbations of spacetime generated by mass distributions with non-null second derivative of the quadrupole mass-moment.
Binary systems of stellar objects are exemplary GW emitters
% change, too similar to thesis
and the more compact the object, the bigger
% in amplitude
the wave produced, making the system more suitable for detection. 

The LIGO-Virgo-KAGRA (LVK) collaboration uses a system of interferometers to detect gravitational waves.
%  and they build up catalogues of gw events.
The latest complete catalogue published by the LVK collaboration is the third gravitational-wave transient catalogue (GWTC-3) and contains about a hundred signals generated by the coalescence of binaries of neutron stars (NSs) and stellar-mass black holes (BHs).

When LVK detect an astrophysical signal, they analyze the data to infer the properties of the event, such as the masses of the objects in the binary, their spins, the distance from the observer and its sky localization.
The statistical framework used for individual event analysis is Bayesian inference.
With the number of events growing, it becomes crucial to do population analyses, as they allow to set further constraints on the astrophysics underlying stellar objects.
To do so, one uses Hierarchical Bayesian statistics.

The models used have multi-dimensional parameters, resulting in probability distributions computationally prohebitive. 
% this sentence is strange, say the problem is the integral
Traditional methods rely on stochastic sampling methods, such as Markov Chain Monte Carlo (MCMC) or Nested sampling, to infer the probability distribution.
However powerful, these methods are expensive, as probability distributions have to be evaluated million of times for each individual event analysis. 
% look for citations
% do we have numbers for populations?
Each evaluation requires to generate a waveform on the fly, which heavily contributes to the overall costs.
% check if true
% si puo dire on the fly su un documento di questo tipo?
Machine learning approaches have the advantage of moving most of the costs in the training stage, dramatically speeding up inference time.
Several attempts have been made in this direction, from architectures that model the noise more realistically than traditional methods, to architectures that employ models without an analytical expression, which is unfeasible with stochastic samplers.
% sei sicura?

\begin{comment}
(nothing technical: that comes later)
- one uses bayesian stat to analyze individual events
- one uses hierarchical to analyze populations
- an alternative approach is simulation based inference
- advantages for individual events
- advantages for populations
\end{comment}

% add some data description? It's nice for the physics but not enough space
\begin{section}{Bayesian statistics}

Let us refer to the vector of binary parameters as $\theta$. It consists of intrinsic parameters, such as the masses and the spins, and extrinsic parameters, such as the redshift and the binary orientation.
The goal of the analysis is to infer the posterior distribution $p(\theta|d)$, which associates to each point in the binary-parameter space the probability of that binary to generate the observed data $d$. 
Observed data is the sum of the astrophysical signal, which is assumed to be deterministic and a function of binary-parameters, and a noise realization in the detector, which is assumed to be stochastic.
To evaluate the posterior one needs a prior $\pi(\theta)$ and a Likelihood $\mathcal{L}(d|\theta)$.
The prior is the probability distribution that an event with parameters $\theta$ takes place in the Universe. 
A possiblility is to use knowledge from theory or previous experiments to build it.
Nevertheless, the preferred approach is often conservative, and uninformative priors are used.
These priors have constant probability in the binary-parameters dominium and zero outside.
The Likelihood is the probability in the data space, given $\theta$ parameters.
To evaluate the Likelihood one needs to generate waveforms for $\theta$, and a noise model, which is often assumed to be stationary and Gaussian.
% non mi piace questa frase
Bayes theorem links the posterior to the prior and the Likelihood:
\begin{equation}
p(\theta|d) \propto \pi(\theta) \times \mathcal{L}(d|\theta).
\end{equation}
\end{section}


\begin{section}{Hierarchical Bayesian statistics}

\end{section}

\begin{comment}
When emitted, they travel through the Universe carrying pristine information on the event producing them.

Since the amplitude of the wave and its evolution depend on the properties of the system, with data analysis we can infer the properties of the system producing the detected wave.

-The following part is not too much impo for my work, so don't deepen it too much-
The first challenge is the detection of signals of astrophysical origin. 
If we consider a binary with ... (add some properties such as mass etc), then the relative change in the length of arms of the interferometer is $\sim 10^{-21}$, which, for a typical intereferometer arm of $5 km$, is ... cms, i.e. x orders of magnitude smaller than the size of an atom (check).
With such a small amplitude of astrophysical signal, the output signal is dominated by noise. 
The procedure LVK uses to detect signals strongly relies on accurate modelling the gravitational-wave signals we are looking for, and then use the models to build filters.
To assess how confident we are that a segment of filtered data contains astrophysical signal, typically one defines a metric such as the signal to noise ratio (SNR) or the false alarm rate (FAR). 
Then one sets a threshold on the metric to decide when to analyze data as containing an event.




\end{comment}



\begin{comment}
Once an astrophysical signal is detected in a data segment, the following step is to analyze the data and with the goal of inferring the properties of the event producing that signal. 

In the signal-detection step, one uses the gravitational-wave signals expected in case of binary mergers to build filters. 
For this reason, during data analysis, we assume that the signal detected comes from a binary merger. (Don't like the for this reason part)
We refer to the parameters used to parametrize binary systems as $\theta$. 
Those are for instance the masses, spins, redshift, sky location parameters.
The output signal $d$ is:
\begin{equation}
d = h(\theta) + n,
\end{equation}
where n is the noise realization and $h(\theta)$ the gravitational signal produced by a binary with parameters $\theta$.
The ultimate goal of the analysis is to infer the posterior, $p(\theta|d)$, which is the probability in the binary-parameter space $\theta$ of generating the observed data $d$.
The Bayes theorem allows us to rewrite the posterior as a function of two distributions that we can evaluate, the Likelihood $\mathcal{L}(d|\theta)$ and the prior $ \pi(\theta)$:
\begin{equation}
p(\theta|d) \propto \pi(\theta) \times \mathcal{L}(d|\theta).
\end{equation}
The prior is the probability distribution that formalizes our knowledge on binary parameters.
Informative priors are built based on theory or previous experiments.
For instance, if we know the distribution of galaxies from electromagnetic observations, we can use this information to build a prior for localization parameters.
Often one prefers a conservative approach using uninformative priors, with uniform probability in the binary-parameters dominium and zero outside.
The prior depends on the physics only, and not on our detector properties.
The Likelihood is a distribution over data, and contains information on the possible detector outputs given an event with parameters $\theta$.
To evaluate the likelihood can be evaluated assuming a noise model and a waveform model.






---------

(Noise is not deterministic, which means we don't care its value at t but we care about is statistical properties -> better to go in the fourier domain. We assume it is gaussian, stochastic and stationary. This means it is completely defined by its autocorrel function and its average)
----------

in analyses we need models to evaluate the likelihood -> expensive
\end{comment}













\end{chapter}

\begin{chapter}{Completed Work}
a descripiton of work completed to date
- small project description

Transfer learning is a series of techniques used in machine learning to transfer knowledge from a neural network to another.
Each time one modifies a neural network, for instance changing the model assumed in the analysis, the neural network needs to be trained again. 
The goal of transfer learning is to fine tune a new training run using some knowledge acquired during a previous training.
These strategies allow to reduce training time, and, more importantly, could even improve the results respect to training the new run without any information from previous trainings. 




\end{chapter}

\begin{chapter}{Work Plan}
a plan of work for the next 12 months
- pop growing pains for sure
- we talked about using astrophysical models as pop models. We need to understand better. Surely something more astro
- another idea is to keep developing the current project (NR simulations, beyond GR models)
\end{chapter}

\begin{chapter}{Personal Development Plan}
Copy of personal development plan summary and a statement of progress made towards those training goals
\end{chapter}

\appendix
%
%\pagestyle{fancy}
%
%\fancyhf{} % Clear all header and footer fields
%\fancyhead[LE,RO]{\small \thepage} % Page number on the outer corners
%\fancyhead[RE]{\small \spacedlowsmallcaps \leftmark} % Chapter title on the right of even pages (LE)
%\fancyhead[LO]{\small \spacedlowsmallcaps \leftmark}
%

\backmatter
%%BIBLIOGRAPHY
\bibliography{thesis}

\end{document}
