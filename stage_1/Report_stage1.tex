\documentclass[11pt,a4paper,oneside]{book}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[]{graphicx}
\usepackage{subfig}
\usepackage{subfig}
\usepackage[]{color}
\usepackage{alltt}
\usepackage{titlesec}
\usepackage{layaureo}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{siunitx} 
\usepackage{amsmath}
\usepackage{array}
\usepackage{comment}
%\usepackage{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{amsmath,amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage{comment}

%\usepackage{subcaption}
%\usepackage{cleveref}


%Journal abbreviations
\def\aj{{Astron.~J.}}
\def\actaa{{Acta Astron.}}\def\araa{{Ann.~Rev.~Astron.~Astrophys.}}
\def\apj{{Astrophys.~J.}}\def\apjl{{Astrophys.~J.~Lett.}}
\def\apjs{{Astrophys.~J.~Supp.}}
\def\ao{{Appl.~Opt.}}
\def\apss{{Ap\&SS}}
\def\aap{{Astron.~Astrophys.}}
\def\aapr{{A\&A~Rev.}}
\def\aaps{{A\&AS}}
\def\azh{{AZh}}
\def\baas{{BAAS}}
\def\bac{{Bull. astr. Inst. Czechosl.}}
\def\caa{{Chinese Astron. Astrophys.}}
\def\cjaa{{Chinese J. Astron. Astrophys.}}
\def\icarus{{Icarus}}
\def\jcap{{J. Cosmology Astropart. Phys.}}
\def\jrasc{{JRASC}}
\def\memras{{MmRAS}}
\def\mnras{{Mon.~Not.~R.~Astron.~Soc.}}             
\def\na{{New A}}
\def\nar{{New A Rev.}}
\def\pra{{Phys.~Rev.~A}}
\def\prb{{Phys.~Rev.~B}}
\def\prc{{Phys.~Rev.~C}}
\def\prd{{Phys.~Rev.~D}}
\def\pre{{Phys.~Rev.~E}}
\def\prx{{Phys.~Rev.~X}}
\def\prl{{Phys.~Rev.~Lett.}}
\def\pasa{{PASA}}
\def\pasp{{PASP}}
\def\pasj{{PASJ}}
\def\rmxaa{{Rev. Mexicana Astron. Astrofis.}}
\def\qjras{{QJRAS}}
\def\skytel{{S\&T}}
\def\solphys{{Sol.~Phys.}}
\def\sovast{{Soviet~Ast.}}\def\ssr{{Space~Sci.~Rev.}}
\def\zap{{ZAp}}
\def\nat{{Nature}}
\def\iaucirc{{IAU~Circ.}}
\def\aplett{{Astrophys.~Lett.}}
\def\apspr{{Astrophys.~Space~Phys.~Res.}}
\def\bain{{Bull.~Astron.~Inst.~Netherlands}}
\def\fcp{{Fund.~Cosmic~Phys.}}
\def\gca{{Geochim.~Cosmochim.~Acta}}
\def\grl{{Geophys.~Res.~Lett.}}
\def\jcp{{J.~Chem.~Phys.}}
\def\jgr{{J.~Geophys.~Res.}}
\def\jqsrt{{J.~Quant.~Spec.~Radiat.~Transf.}}
\def\memsai{{Mem.~Soc.~Astron.~Italiana}}
\def\nphysa{{Nucl.~Phys.~A}}
\def\physrep{{Phys.~Rep.}}
\def\physscr{{Phys.~Scr}}
\def\planss{{Planet.~Space~Sci.}}
\def\procspie{{Proc.~SPIE}}
\def\lrr{{Living Rev. Relativ.}}

\raggedbottom
\geometry{a4paper,top=4cm,bottom=3.5cm}

\numberwithin{equation}{chapter}
\numberwithin{table}{chapter}
\numberwithin{figure}{chapter}


%Bibliography
%\usepackage{csquotes}
%\usepackage[maxbibnames=9, minbibnames=4, style=numeric, backend=bibtex8]{biblatex}
%\addbibresource{biblio.bib}
\usepackage[square, numbers, sort&compress]{natbib} 
\bibliographystyle{apsrev4-1}
\setcitestyle{numbers}
%\appto{\bibsetup}{\raggedright}


\title{Annual Progression Review}
\author{Cecilia Maria Fabbri}
\date{1 Steptember 2025}	

\begin{document}
\bibliographystyle{plain}

\maketitle
\tableofcontents

\begin{chapter}{Literature Review}
\begin{comment}
- intro gw super quick -> we have a signal! (add if you have space: good candidates to detect are binary systems, detectors, current catalogue description)
- data analysis for single event
- data analysis for populations
- sbi basics, single event
- sbi population
- growing pains (accuracy requirements)
--------------------
\end{comment}

Gravitational waves (GWs) are perturbations of spacetime generated by mass distributions with a non-vanishing second derivative of the quadrupole mass-moment.
Binary systems of stellar objects are exemplary GW emitters
% change, too similar to thesis
and the more compact the object, the larger
% in amplitude
the wave amplitude, making the system more suitable for detection \cite{2014LRR....17....3P}. 

The LIGO-Virgo-KAGRA (LVK) collaboration uses a system of interferometers to detect gravitational waves \cite{{2016PhRvL.116m1103A},{2016PhRvD..93l2003A},{2016PhRvL.116f1102A}}.
The most recent complete catalogue they published is the third gravitational-wave transient catalogue (GWTC-3), which contains about a hundred signals generated by the coalescence of binaries of neutron stars (NSs) and stellar-mass black holes (BHs) \cite{2023PhRvX..13a1048A}.

When LVK detects an astrophysical signal, it analyzes the data to infer the properties of the event, such as the masses of the binary components, their spins, the distance from the observer and the sky localization.
The statistical framework used for individual event analysis is Bayesian inference.
As the number of detections is growing, population analyses become crucial, as they enable further constraints on the astrophysics of compact objects (COs) \cite{{2023PhRvX..13a1048A},{2017CQGra..34cLT01V},{2017PhRvD..96b3012T},{2017PhRvD..95l4046G}}.
To this end, one uses Hierarchical Bayesian statistics \cite{{2021ApJ...913L...7A},{2020PASA...37...36T}}.

The models used have multi-dimensional parameters, resulting in probability distributions computationally prohibitive. 
% this sentence is strange, say the problem is the integral
Traditional approaches rely on stochastic sampling methods (e.g. Markov Chain Monte Carlo (MCMC) or Nested sampling ) to infer probability distributions \cite{{2020PASA...37...36T},{2019ApJS..241...27A},{2020MNRAS.493.3132S},{2020MNRAS.499.3295R}}.
However powerful, this approach is computationally expensive, as probability distributions are evaluated iteratively million of times for each analysis. 
% look for citations
% do we have numbers for populations?
Evaluations require to generate a waveform on the fly, which substantially contributes to the overall computational cost.
% check if true
% si puo dire on the fly su un documento di questo tipo?
Machine learning approaches have the advantage of shifting most of the cost 
%offline
to the training stage, dramatically speeding up inference \cite{{2025PhRvD.111l3049M},{2020arXiv200803312G},{2021PhRvL.127x1103D}}.
%, which is crucial for multimessanger astronomy.
%Several attempts have been made in this direction, ranging from architectures that model noise more realistically than traditional methods to architectures that employ complex physical models without requiring an analytical expression.
%, which is unfeasible with stochastic samplers.

\begin{section}{Bayesian statistics}

Let us refer to the vector of binary parameters as $\theta$. It consists of intrinsic parameters, such as the masses and spins, and extrinsic parameters, such as the redshift and binary orientation.
The goal of the analysis is to infer the posterior distribution $p(\theta|d)$, which assigns to each point in the binary-parameter space the probability that binary the corresponding binary generated the observed data $d$. 
The observed data is the sum of the astrophysical signal, which is assumed to be deterministic and a function of binary-parameters, and a noise realization in the detector, which is assumed to be stochastic.
To infer the posterior one needs a prior $\pi(\theta)$ and a likelihood $\mathcal{L}(d|\theta)$ \cite{2013PhRvD..87l2003C}.
The prior is the probability distribution that an event with parameters $\theta$ occurs in the Universe. 
A possible approach is to construct the prior based on theory or previous experiments.
Nevertheless, the preferred approach is often conservative, using uninformative priors, such as priors with constant probability inside the binary-parameters domain and zero outside.
The likelihood is the probability in the data space, given parameters $\theta$.
To evaluate the likelihood one needs to generate waveforms for $\theta$ and a noise model \cite{{2015PhRvD..91h4034L},{2011CQGra..28a5010R}}.
To construct the noise model one often assumes that noise is stationary and Gaussian.
% non mi piace questa frase
Bayes' theorem links the posterior to the prior and the likelihood:
\begin{equation}
\label{eq: post}
p(\theta|d) = \frac{ \pi(\theta) \times \mathcal{L}(d|\theta)}{p(d)}.
\end{equation}
% non mi piace links in questo contesto
The term $p(d)$ is called evidence, and it is the probability of observing the data $d$ under the assumed model.
The evidence is defined as
\begin{equation}
p(d) = \int\textrm{d}\theta \pi(\theta) \times \mathcal{L}(d|\theta).
\end{equation}
The evidence acts as a normalization factor and does not affect the posterior shape.
For this reason, it is typically not computed during inference, except when performing model comparison.


\end{section}


\begin{section}{Hierarchical Bayesian statistics}
The goal of population analysis is to infer how binary parameters are distributed at the population level. 
First, one parametrizes the distribution of binary parameters with the population parameters $\lambda$, which determine the shape of the distribution. 
Examples of such hyperparameters include the slope of the binary black hole (BBH) mass distribution, as well as the minimum and maximum mass of stellar-mass BHs.
The prior on binary parameters is conditioned on the population parameters, and we refer to it as the population model $p_{\rm pop}(\theta|\lambda)$.
For the analysis, one also needs the prior on population parameters, called hyperprior, $\pi(\lambda)$.

The choice of the population model is crucial, and several approaches have been proposed to construct it.
The state-of-the-art methods are parametric models, which rely on simple parametric forms motivated by qualitative astrophysical arguments \cite{{2019ApJ...882L..24A},{2021ApJ...913L...7A},{2023PhRvX..13a1048A}}.
Other approaches move toward more flexible models, referred to as semi-parametric or non-parametric models, where the hyperparameters are for instance spline knots and perturbations \cite{{2022MNRAS.509.5454R}, {2023ApJ...946...16E}, {2024PhRvX..14b1005C}}. These models enable data-driven fits but their results are often more difficult to interpret physically.
Astrophysical approaches often consist of comparing observed data with the output of simulations that synthesize binary populations \cite{{2022ApJS..258...34R},{2020ApJ...898...71B},{2018PhRvD..98h3017T},{2022PhRvD.106j3013M}}.

In population analyses, it is necessary to take selection effects  into account \cite{{2022hgwa.bookE..45V},{2019MNRAS.486.1086M}, {2024CQGra..41l5002G}}.
Since different events have different probabilities of being detected, the observed population is biased.
An event is considered detectable if the data exceeds a given threshold.
The detection probability for an event is defined as
\begin{equation}
p_{\rm det}(\theta) = \int_{d>treshold} \textrm{d}d p(d|\theta).
\end{equation}

Let us refer to the dataset of observed events as $d$ and to individual events as $d_{i}$, with $i=1, ..., N_{\rm obs}$, where $N_{\rm obs}$ is the number of observed events.
The posterior for the astrophysical distribution, called the hyperposterior, is:
\begin{equation}
\label{eq: hyperpost}
p(\lambda|d) = \frac{\pi(\lambda)}{p(d)} \prod_{i=1}^{N_{\rm obs}}\frac{\int \textrm{d}\theta \mathcal{L}(d_{i}|\theta)p_{\rm pop}(\theta|\lambda)}{\int \textrm{d}\theta p_{\rm pop}(\theta|\lambda)p_{\rm det}(\theta)},
\end{equation}
where $\mathcal{L}(d_{i}|\theta)$ is the likelihood of individual events as defined in Eq.~\ref{eq: post}.
The term $p(d)$ is the hyperevidence, and it is the probability of the observed dataset under the assumed population model, i.e.:
\begin{equation}
p(d)=\int \textrm{d}\lambda'  \pi(\lambda') \prod_{i=1}^{N_{\rm obs}} \mathcal{L}(d_{i}|\lambda').
\end{equation}
Here, the likelihood respect to the hyperparameters $\mathcal{L}(d_{i}|\lambda')$ is related to the individual-event likelihood through
\begin{equation}
\mathcal{L}(d_{i}|\lambda') = \int \textrm{d}\theta\mathcal{L}(d_{i}|\theta)p_{\rm pop}(\theta|\lambda).
\end{equation}
The hyperevidence is often computed in population analysis to compare different population models and assess how well they describe the observed dataset. 

Population analyses are typically carried out in two steps.
First, individual events are analyzed using Bayesian statistics. 
Then, one uses the resulting individual-event posteriors and the assumed population model to infer the hyperposterior in Eq.~\ref{eq: hyperpost}.

\end{section}



\begin{section}{Simulation based inference}
%Traditional methods that use stochastic sampling infer the posterior by (i) drawing events randomly from the prior and (ii) evaluating the Likelihood.
%One can use deep learning to model complex probability distributions.
Simulation-based inference (SBI) is a deep learning technique for approximating probability distributions using simulated data \cite{{2016arXiv160506376P},{2020PNAS..11730055C},{2017arXiv171101861L},{2019arXiv190507488G}}.
I am working with the DINGO code, which applies SBI to model posteriors of gravitational-wave data \cite{{2020PhRvD.102j4057G},{2020arXiv200803312G},{2021PhRvL.127x1103D},{2023PhRvL.130q1403D}}.
To simulate data, we need to draw samples from the prior and the likelihood.
Unlike to traditional methods, SBI does not require explicit evaluation of the likelihood. 
%nor an analytical expression for it. 
In principle, more complex models that lack an analytical expression can be used.
For instance, one can relax the assumptions of stationary and gaussian noise, or use waveforms generated from simulations \cite{{2025PhRvD.112b4071M}, {2025ApJ...985L..46L}}.

Normalizing flows are often used in SBI to approximate complex distributions.
A normalizing flow is a mapping from a simple $\pi(u)$ to an approximation of the posterior $q_{\phi}(\theta|d)$, where $\phi$ are the parameters of the mapping. 
The mapping $f_{\phi}(u)$ must be invertible and with a simple Jacobian determinant, so that:
\begin{equation}
\label{eq: nf}
q_{\phi}(\theta|d) = \pi(f_{\phi}^{-1}(\theta))\left|\textrm{det}J_{f_{\phi}}^{-1}\right|.
\end{equation}
% long |
% what is the dependence of the flow on data?
% add phi and explain
To evaluate $q_{\phi}(\theta|d)$ one uses Eq.~\ref{eq: nf}. Drawing samples from $q_{\phi}(\theta|d)$ is equivalent to drawing $u\sim\pi(u)$ and transforming $\theta=f_{\phi}(u)$.
One often chooses the base distribution $\pi(u)$ to be a multivariate standard normal.
In the case of DINGO, the flow $f_{\phi}$ is a sequence of transforms $f_{\phi}^{(i)}$, such that
\begin{equation}
f_{\phi}^{i}(u) = 
\begin{cases}
u_{i} & \text{if } i\leq d/2 \\
c_{i}(u_{i}; u_{1:{d/2}},d) & \text{if } i> d/2
\end{cases}.
\end{equation}
Half of the latent variables $u$ remain unchanged, while the other half are transformed.
At each $f_{\phi}^{(i)}$ the indices are permuted.
The transformation used is the neural spline coupling transform $c_{i}$, whose parameters depend on the unchanged $u_{i}$ and on data strain $d$.

During training, the neural network learns the mapping by optimizing the parameters $\phi$ to minimize a loss function.
In these problems, the loss functions is typically based on the Kullback-Leibler divergence, defined as:
\begin{equation}
D_{\rm KL}(p||q_{\phi}) = \int \textrm{d}\theta p(\theta|d) \log{\frac{p(\theta|d)}{q_{\phi}(\theta|d)}},
\end{equation}
which vanishes as the approximant becomes closer to the true posterior.
Since the approximant should not depend on the individual data, we define the loss function as the expectation value over $p(d)$ of $D_{\rm KL}(p||q_{\phi})$:
\begin{equation}
L  = \int \textrm{d}{d}p(d)\int \textrm{d}\theta p(\theta|d) \log{\frac{p(\theta|d)}{q_{\phi}(\theta|d)}}.
\end{equation}
Using Bayes Theorem one obtains:
\begin{equation}
L  \simeq - \int \textrm{d}\theta \pi(\theta) \int\textrm{d}{d} p(d|\theta)\log{q_{\phi}(\theta|d)}, 
\end{equation}
where the posterior at the numerator of the logarithm can be omitted since the minimization over $\phi$ does not depend on the true posterior.
Simulating events is equivalent to drawing $N$ events from the prior, $\theta_{i}\sim \pi(\theta)$, and data from the likelihood $d_{i} \sim p(d|\theta_{i})$, with $i=1,...,N$.
The loss is then given by
\begin{equation}
L = - \frac{1}{N} \sum_{i} \log{q_{\phi}(\theta|d)},
\end{equation}
where Monte Carlo (MC) sums are used to approximate the integrals \cite{10.5555/2578955}.

DINGO architecture uses an embedding network to compress data strains before passing them to the flow \cite{2021PhRvL.127x1103D}. 
We train the embedding jointly with the flow. 
During training the network learns how to compress the data, i.e. it identifies the most important features.
% say that it's a residual network

\end{section}














\end{chapter}

\begin{chapter}{Completed Work}
Transfer learning is a set of techniques in machine learning used to transfer knowledge from one neural network to another.
%These strategies allow to reduce training time, and, more importantly, could even improve the results respect to training the new run without any information from previous trainings. 

My first projects involves testing transfer learning with DINGO for individual-events analysis, following the multifidelity approach in Ref.~\cite{2025arXiv250208416K}.
The typical training dataset size required to obtain well-trained NNs in DINGO is of the order of $10^{6}$ simulations.
Let us consider two waveform models.
A high-fidelity model is accurate but computational expensive, with prohibitive costs that limit the number of simulations to well below $10^{6}$ (e.g. numerical relativity simulations).
A low-fidelity model is less accurate and cheap, allowing us to easily simulate $10^{6}$ data.
Assuming that the flow architecture is the same in both cases, we fine-tune the high-fidelity network using the parameters optimized during the low-fidelity training run.
The NN for the high-fidelity model achieves higher performances when pretrained, partially inheriting knowledge acquired during the low-fidelity training.




\begin{section}{Diagnostics to check the performance of the neural network}

There are several diagnostics to assess the quality of the posterior approximant.

The evolution of the loss function provides information on how training is progressing.
When training a NN, the dataset is split into a training set and validation set. 
The training set is used to update the NN parameters in order to minimize the loss.
The validation set is used to compute the loss after each update, allowing one to monitor the NN's performance on unseen data.
A common sanity check consists in ensure that the validation and training losses decrease together during training.
By the end of training, both losses should reach a plateau.

%Once training is finished, we can test the approximant using injections.
\begin{comment}
We use pp-plots to test the one-dimensional marginal posteriors.
For each one-dimensional marginal of $q_{\phi}(\theta|d)$, one expects the cumulative distribution of the percentiles of true parameters to align to a diagonal as the approximant gets closer to the true posterior marginals.
\end{comment}
Importance sampling tests how closely $q_{\phi}(\theta|d)$ approximates $p(\theta|d)$ in multiple dimensions.
During inference one draws samples $\theta_{i}$ from the approximant of the posterior $\theta_{i}\sim q_{\phi}(\theta|d)$.
Each $\theta_{i}$ can be assigned a weight $w_{i}$:
\begin{equation}
w_{i} \propto \frac{\pi(\theta_{i})p(d|\theta_{i})}{q_{\phi}(\theta_{i}|d)}.
\end{equation}
The variance of weights provides a measure of the sampling efficiency
\begin{equation}
\epsilon = \frac{\left(\sum_{i}w_{i}\right)^{2}}{n\sum_{i}w_{i}^{2}}
\end{equation}
\cite{2023PhRvL.130q1403D}.
Smaller values of $\epsilon$ correspond to poorer posterior approximations.


\end{section}


\begin{section}{From non-spinning to aligned-spins model}
To test the multifidelity procedure, we use a non-spinning waveform as the low-fidelity model, while the high-fidelity model includes aligned spins.
We use IMRPhenomXPHM as the approximant to generate waveforms is .
% cita IMRPhenomXPHM
%For the low-fidelity model we generate $5\times10^6$, with a split of $95\%$ training set and $5\%$ validation set. For the high-fidelity model, we generate multiple datasets of sizes $5\times10^6$, $5\times10^5$, $5\times10^4$, and $8192$. In the first three cases the split is the same as in the low-fidelity case, while in the latter case we split the dataset in two halves.
For the low-fidelity model, we generate $\sim 10^{6}$ simulations.
For the high-fidelity model, we generate multiple datasets of sizes $\sim 10^{5}$, $\sim 10^{4}$ and $\sim 10^{3}$.
For the datasets of $\sim10^{6}$ and $\sim10^{3}$ simulations, we also perform additional training where we pretrain the embedding only.
Results for pretraining only the embedding are preliminary.
We plan to explore this approach further, as it could improve the flexibility of the high-fidelity model, allowing us to use pretraining while modifying the flow architecture.

Figure ~\ref{loss} shows the evolution of the validation and set loss functions during training.
In the two cases with largest number of simulations ($\sim 10^{6}$ and $\sim 10^{5}$), pretraining both the embedding and the flow does not significantly affect the final loss plateau. The main advantage of pretraining is the reduced time to reach the plateau, resulting in lower computational costs.
For limited simulation budgets ($\sim 10^{4}$ and $\sim 10^{3}$), pretraining reduces the loss to lower values than traditional training.
Pretraining the embedding alone with $\sim10^{6}$ simulations allows a slightly lower final loss compared to training from scratch.
For $\sim10^{3}$ simulations, this strategy improves results over traditional training but does not achieve the performance of pretraining both the embedding and the flow.

The cornerplot in Fig.~\ref{cp} shows the analysis of an injection for different simulation budgets when pretraining both the flow and the embedding. 
The marginal posteriors are consistent across budgets. However, the marginal posterior for $\chi_{1}$ deviates respect to the injected value, while the marginal posterior for $\chi_{2}$ is close to the prior. 

The sampling efficiencies for $1000$ injections (Fig.~\ref{eff}) confirm that pretraining with $\sim10^{4}$ and  $\sim10^{3}$ simulations improves performance. 
The histogram in Fig.~\ref{eff} shows that even with pretraining, the $\sim10^{3}$ simulation case has considerably lower efficiencies than the other cases, with most injections having $\epsilon<1\%$.
Contrarily to expectations, Fig.~\ref{eff} shows no correlation between sampling efficiency and the higher spin magnitude.
%sesnibly is okay in this context?





%Hinting to the fact that the spin components might not be learned properly.
% comment on the fact that this might be due to the dataset size which is unbalanced towards low spins



% better results does not mean good results -> check neff

\begin{figure}
\includegraphics[width=1.\columnwidth]{figures/val_loss.pdf}
\caption{Evolution of the validation loss during training. The number of epochs corresponds to the number of times the full training set passes through the neural network. The top two panels show the losses for traditional training (blue) and pretraining both the embedding and the flow (orange) for $\sim10^{5}$ (left) and $\sim10^{4}$ (right) simulations. The bottom two panels show the validation losses for  $\sim10^{6}$ (left) and $\sim10^{3}$ (right) simulations. In addition to the previous cases, results for pretraining the embedding only (green) are also shown.}
\label{loss}
\end{figure}

\begin{figure}
\includegraphics[width=1.1\columnwidth]{figures/cornerplots.pdf}
\caption{Cornerplots showing the one-dimensional (diagonal) and two-dimensional (off-diagonal) posteriors for binary parameters given an injected signal (black lines and markers). Results are obtained using DINGO with pretraining of both the embedding and the flow, with training datasets of order of magnitude $\sim10^{6}$ (blue), $\sim10^{5}$ (orange), $\sim10^{4}$ (green), $\sim10^{3}$ (red).}
\label{cp}
\end{figure}

\begin{figure}
\includegraphics[width=1.1\columnwidth]{figures/neff.pdf}
\caption{Scatter plots of sampling efficiecies  for $10^{3}$ injected signals, comparing pretraining (x axis) and no pretraining (y axis). The black dotted line corresponds to $y=x$; injections below the line have higher efficiency with pretraining. Markers are colored based on the higher-spin value. The top four plots show results when pretraining both the embedding and the flow for different simulation budgets, while the bottom-left plot shows results when pretraining the embedding only ($\sim 10^{6}$ simulations). The bottom-right panel displays histograms of the sampling efficiencies for pretraining cases.}
\label{eff}
\end{figure}







\end{section}



\end{chapter}

\begin{chapter}{Work Plan}
In the coming months I will continue working on the pretraining project, focusing on whether spin marginals are properly learned.
%There are different questions that need further exploration:
%\begin{itemize}
%\item for limited simulations budgets, results are better with pretraining than without, but this does not guarantee results' reliabilty. Sampling efficiency allows to assess the goodness of fit. However, small training datasets often come from non-analytical models, that would make sampling efficiency unfeasible. How do we go beyond sampling efficiency and how do we asses the posterior reliability across the parameter space?
%\item Are spin marginals learned properly? Even if the scatterplots (Fig.~\ref{eff}) do not show a decrease in sampling efficiency for high spin binaries, cornerplots (Fig.~\ref{cp}) show some ill-behaviour in inferring spins.
%\end{itemize}
%Other interesting applications of this project would be using numerical relativity simulations as dataset.
With pretraining we can analyze events with numerical relativity waveform models, using, for instance, beyond general relativity (GR) models, or precessing and eccentric waveforms.

Some attempts are being to apply machine learning to populations \cite{{2024PhRvD.109f4056L}, {2025PhRvD.111l3049M}, {2025ApJ...988..189C}}.
One question we aim to address is whether SBI could solve the "growing pains" problem \cite{2023MNRAS.526.3495T}.
This issue arises because MC integrals have an intrinsic variance.
In traditional methods likelihoods are estimated through MC integrals, which introduces an uncertainty that grows with the dataset size, and can bias future analyses. 
Another possible direction is to apply pretraining in population analyses, using population-synthesis codes to generate the dataset. 
This would allow us to perform inference directly on astrophysical processes, since population parameters would be the input parameters of the codes, e.g. the supernova kick strength, common-envelope efficiency, metallicity, and winds strength.



%One of the open problems in the field is how to compare data with the results of population-synthesis codes.
%One of the possible projects is to analyze data using these codes to generate the training dataset, i.e. use astrophysical models to do direct fit of the data.
%Another project regards exploring how the "growing pains" problem is projected in the SBI context.





\end{chapter}

\begin{chapter}{Personal Development Plan}
Below I attach the DNA form. 

In the last months I deepened my knowledge on machine learning, particularly in SBI.

I have attended and passed two exams: one specifically on machine learning, and one on extreme astrophysics.
These exams taught me about a variety of possible machine learning architectures, used in many fields also beyond astrophysics, as well as about astrophysical processes in galaxies.

I have also attended four conferences, three on general relativity (Nottingham, Birmingham and Glasgow, UK) and one on machine learning applied to gravitational-wave analyses (Providence, US). 
In Birmingham I gave a presentation, while in Glasgow and in Providence I presented a poster.

\end{chapter}

\appendix
%
%\pagestyle{fancy}
%
%\fancyhf{} % Clear all header and footer fields
%\fancyhead[LE,RO]{\small \thepage} % Page number on the outer corners
%\fancyhead[RE]{\small \spacedlowsmallcaps \leftmark} % Chapter title on the right of even pages (LE)
%\fancyhead[LO]{\small \spacedlowsmallcaps \leftmark}
%


% add some data description? It's nice for the physics but not enough space

%\printbibliography
% \bibliographystyle{plain}
 \bibliography{biblio}


\end{document}
